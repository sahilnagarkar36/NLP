{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VW73dVI9ghZC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import random\n",
        "from collections import defaultdict, Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NGramAutoComplete:\n",
        "    def __init__(self, corpus, n=3):\n",
        "        \"\"\"Initializes the N-gram model for auto-completion.\"\"\"\n",
        "        self.n = n\n",
        "        self.ngrams = defaultdict(Counter)\n",
        "        self.build_model(corpus)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Preprocesses text by converting to lowercase and tokenizing.\"\"\"\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text.lower())\n",
        "        return text.split()\n",
        "\n",
        "    def build_model(self, corpus):\n",
        "        \"\"\"Builds an N-gram frequency model from the given corpus.\"\"\"\n",
        "        tokens = self.preprocess_text(corpus)\n",
        "        for i in range(len(tokens) - self.n + 1):\n",
        "            context = tuple(tokens[i:i+self.n-1])\n",
        "            next_word = tokens[i+self.n-1]\n",
        "            self.ngrams[context][next_word] += 1\n",
        "\n",
        "    def predict_next_word(self, context):\n",
        "        \"\"\"Predicts the most probable next word based on the given context.\"\"\"\n",
        "        context = tuple(self.preprocess_text(context)[-self.n+1:])\n",
        "        if context in self.ngrams:\n",
        "            return self.ngrams[context].most_common(1)[0][0]\n",
        "        return None\n",
        "\n",
        "    def generate_text(self, seed_text, max_words=10):\n",
        "        \"\"\"Generates text by predicting words iteratively.\"\"\"\n",
        "        words = self.preprocess_text(seed_text)\n",
        "        for _ in range(max_words):\n",
        "            next_word = self.predict_next_word(' '.join(words))\n",
        "            if not next_word:\n",
        "                break\n",
        "            words.append(next_word)\n",
        "        return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "bm381SGIhoKB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_corpus = \"The quick brown fox jumps over the lazy dog. The quick brown cat sleeps under the big tree.\""
      ],
      "metadata": {
        "id": "eOPtF3YIhrmg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auto_complete_model = NGramAutoComplete(dummy_corpus, n=3)"
      ],
      "metadata": {
        "id": "s3errBwkhu_R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"The quick brown\"\n",
        "predicted_word = auto_complete_model.predict_next_word(context)\n",
        "print(f\"Predicted next word: {predicted_word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSf1mBIthxdI",
        "outputId": "8b019fd2-4439-413a-9ac0-5295d9732ebe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted next word: fox\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = auto_complete_model.generate_text(\"The quick brown\", max_words=5)\n",
        "print(f\"Generated text: {generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiSNmG_Ehzwo",
        "outputId": "947f9b3c-f892-4500-b994-615a2eb8ec87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text: the quick brown fox jumps over the lazy\n"
          ]
        }
      ]
    }
  ]
}